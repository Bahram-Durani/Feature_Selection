# Feature Selection

Two things distinguish top data scientists from others in most cases: Feature Creation and Feature Selection. i.e., creating features that capture deeper/hidden insights about the business or customer and then making the right choices about which features to choose for your model.

#### Importance of Feature Selection in Machine Learning
Quality of Machine Learning model depends upon your data â€” Garbage in, Garbage out. (Garbage here would mean bad data/noise in data).

After an extensive Feature Engineering step, you would end up with a large number of features. You may not use all the features in your model. You would be interested in feeding your model only those significant features or remove the ones that do not have any predictive power.

Feature Selection diffrent methods:

1. Forward feature selection 
2. Backward feature elimination 

You can find all these 4 under feature_selection_techniques.ipynb file

3. Constant Feature
4. Quasi Constant Feature
5. Chi-Square (Categorical attributes)
6. Correlation Matrix (Numerical Attributes)


